{"cells":[{"metadata":{},"cell_type":"markdown","source":"## VAE method for Motion prediction"},{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch imports\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\nimport torch.functional as F\n\n# l5kit imports\nimport l5kit\nfrom l5kit.configs import load_config_data\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\nfrom l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\nfrom l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\nfrom l5kit.geometry import transform_points\nfrom l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n\n# common imports\nimport os\nimport random\nimport time\nimport pandas as pd\nfrom typing import Dict\nfrom tempfile import gettempdir\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nfrom prettytable import PrettyTable\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l5kit.__version__","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"'1.1.0'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"False"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR_INPUT = '../input/lyft-motion-prediction-autonomous-vehicles'\n# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\ndm = LocalDataManager(None)\nVALIDATION = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg = {\n    'format_version': 4,\n    'model_params': {\n        'model_architecture': 'resnet18',\n        \n        'history_num_frames': 10,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        \n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n    \n    'raster_params': {\n        'raster_size': [1, 1],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map/aerial_map.png',\n        'semantic_map_key': 'semantic_map/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    \n    'train_data_loader': {\n        'key': 'scenes/train.zarr',\n        'batch_size': 12,\n        'shuffle': True,\n        'num_workers': 0\n    },\n    \n    'val_data_loader': {\n        'key': 'scenes/validate.zarr',\n        'batch_size': 12,\n        'shuffle': False,\n        'num_workers': 0\n    },\n    \n    'test_data_loader': {\n        'key': 'scenes/test.zarr',\n        'batch_size': 12,\n        'shuffle': False,\n        'num_workers': 0\n    },\n    \n    'train_params': {\n        'checkpoint_every_n_steps': 1000,\n        'max_num_steps': 3500,\n        'eval_every_n_steps': 500\n    }\n}","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===== INIT DATASET\ntrain_cfg = cfg[\"train_data_loader\"]\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\n# Train dataset/dataloader\ntrain_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\ntrain_dataset = AgentDataset(cfg, train_zarr, rasterizer)\ntrain_dataloader = DataLoader(train_dataset,\n                              shuffle=train_cfg[\"shuffle\"],\n                              batch_size=train_cfg[\"batch_size\"],\n                              num_workers=train_cfg[\"num_workers\"])\n\nprint(train_dataset)","execution_count":28,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-3e1fcdc492a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Rasterizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrasterizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_rasterizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train dataset/dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/usr/lib/lyft_l5kit_unofficial_fix/l5kit/rasterization/rasterizer_builder.py\u001b[0m in \u001b[0;36mbuild_rasterizer\u001b[0;34m(cfg, data_manager)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmap_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"py_semantic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             return SemBoxRasterizer(\n\u001b[0;32m--> 163\u001b[0;31m                 \u001b[0mrender_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_agents_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_num_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantic_map_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_to_ecef\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/usr/lib/lyft_l5kit_unofficial_fix/l5kit/rasterization/sem_box_rasterizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, render_context, filter_agents_threshold, history_num_frames, semantic_map_path, world_to_ecef)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_rast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoxRasterizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_agents_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_num_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msat_rast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSemanticRasterizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantic_map_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_to_ecef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     def rasterize(\n","\u001b[0;32m/kaggle/usr/lib/lyft_l5kit_unofficial_fix/l5kit/rasterization/semantic_rasterizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, render_context, semantic_map_path, world_to_ecef)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto_API\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMapAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_map_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_to_ecef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# TODO is this the right place for this function?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/usr/lib/lyft_l5kit_unofficial_fix/l5kit/rasterization/semantic_rasterizer.py\u001b[0m in \u001b[0;36mget_bounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto_API\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_lane\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mlane\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto_API\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lane_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0mx_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xyz_left\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xyz_right\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0my_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xyz_left\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xyz_right\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/usr/lib/lyft_l5kit_unofficial_fix/l5kit/data/map_api.py\u001b[0m in \u001b[0;36mget_lane_coords\u001b[0;34m(self, element_id)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mright_boundary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertex_deltas_y_cm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mright_boundary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertex_deltas_z_cm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mlane\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeo_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         )\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/usr/lib/lyft_l5kit_unofficial_fix/l5kit/data/map_api.py\u001b[0m in \u001b[0;36munpack_deltas_cm\u001b[0;34m(self, dx, dy, dz, frame)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mframe_lat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_lng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_undo_e7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat_e7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_undo_e7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlng_e7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mxyz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menu2ecef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_lat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_lng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mxyz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mecef_to_world\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxyz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/kaggle/usr/lib/lyft_l5kit_unofficial_fix/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===== INIT  VAL DATASET\nval_cfg = cfg[\"val_data_loader\"]\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\n# Train dataset/dataloader\nval_zarr = ChunkedDataset(dm.require(val_cfg[\"key\"])).open()\nval_dataset = AgentDataset(cfg, val_zarr, rasterizer)\nval_dataloader = DataLoader(val_dataset,\n                              shuffle=val_cfg[\"shuffle\"],\n                              batch_size=val_cfg[\"batch_size\"],\n                              num_workers=train_cfg[\"num_workers\"])\n\nprint(val_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTM Encoder Decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderLSTM(nn.Module):\n    \n    def __init__(self, cfg, nz):\n        super(EncoderLSTM, self).__init__()\n        \n        self.input_sz  = 2\n        self.hidden_sz = nz\n        self.num_layer = 1 # maybe 50\n        \n        self.Encoder_lstm = nn.LSTM(self.input_sz,self.hidden_sz,self.num_layer,batch_first=True)\n       \n    def forward(self,inputs):\n        \n        output,hidden_state = self.Encoder_lstm(inputs)\n        \n        return output,hidden_state\n    \nclass DecoderLSTM(nn.Module):\n    def __init__(self, cfg, nz):\n        super(DecoderLSTM, self).__init__()\n        \n        self.input_sz  = nz\n        self.hidden_sz = nz\n        self.hidden_sz_en = nz\n        self.num_layer = 1 # maybe 50\n        self.sequence_len_de = 1\n        \n        self.interlayer = nz * 4\n\n        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n        \n        self.Decoder_lstm = nn.LSTM( self.input_sz,self.hidden_sz,self.num_layer,batch_first=True)\n\n        \n        self.fcn_en_state_dec_state= nn.Sequential(nn.Linear(in_features=self.hidden_sz_en, out_features=self.interlayer),\n                            nn.ReLU(inplace=True),\n                            nn.Linear(in_features=self.interlayer, out_features=num_targets))\n\n    def forward(self,inputs, hidden_state, z):\n                \n        inout_to_dec = torch.ones(inputs.shape[0],self.sequence_len_de,self.input_sz).to(device)\n\n        #for i in range(cfg[\"model_params\"][\"future_num_frames\"]+1): # this can be used to feed output from previous LSTM to anther one which is stacked.\n        inout_to_dec,hidden_state   = self.Decoder_lstm(inout_to_dec,(hidden_state,z) )          \n        \n        fc_out = self.fcn_en_state_dec_state (inout_to_dec.squeeze(dim=0))\n        \n        return fc_out.reshape(inputs.shape[0],cfg[\"model_params\"][\"future_num_frames\"],-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kl_divergence(mu1, log_sigma1, mu2, log_sigma2):\n  \"\"\"Computes KL[p||q] between two Gaussians defined by [mu, log_sigma].\"\"\"\n  return (log_sigma2 - log_sigma1) + (torch.exp(log_sigma1) ** 2 + (mu1 - mu2) ** 2) \\\n               / (2 * torch.exp(log_sigma2) ** 2) - 0.5\n\n\nclass VAE(nn.Module):\n  def __init__(self, nz, cfg, beta=0.1):\n    super().__init__()\n    self.beta = beta          # factor trading off between two loss components\n    self.cfg = cfg\n\n    self.encoder = EncoderLSTM(cfg, nz * 2)\n    self.decoder = DecoderLSTM(cfg, nz)\n\n  def reparameterize(self, mu, logvar):   \n      if self.training:\n        std = torch.exp(logvar)\n        esp = torch.randn(*mu.size()).to(device)\n        z = mu + (std * esp)\n        return z\n      else:\n        return mu\n\n  def forward(self, inputs):\n    # encode input into posterior distribution q(z | x)\n    _,hidden_state = self.encoder(inputs)\n\n    hidden_state_zero_reshaped = hidden_state[0].squeeze(0)\n    hidden_state_one_reshaped = hidden_state[1].squeeze(0)\n\n    mu0 = hidden_state_zero_reshaped[:,:nz]\n    logvar0 = hidden_state_zero_reshaped[:,nz:]\n\n    mu = hidden_state_one_reshaped[:,:nz]\n    logvar = hidden_state_one_reshaped[:,nz:]\n\n    q = {}\n    q['mean'] = mu\n    q['log_sigma'] = logvar\n\n    # sample latent variable z with reparametrization\n    z0 = self.reparameterize(mu0, logvar0)\n\n    z = self.reparameterize(mu, logvar) # batch of sampled embeddings\n    \n    # compute reconstruction\n    reconstruction = self.decoder(inputs, z0.unsqueeze(0), z.unsqueeze(0))\n\n    return {'q': q, \n          'rec': reconstruction}\n\n  def loss(self, x, outputs, target_availabilities):\n    # compute reconstruction loss    \n    loss = nn.MSELoss(reduction='none')\n\n    rec_loss = loss(outputs['rec'], x)\n    rec_loss = rec_loss * target_availabilities\n\n    mu = outputs['q']['mean']\n    log_sigma = outputs['q']['log_sigma']\n\n    mu2 = torch.zeros_like(mu).to(device)\n    log_sigma2 = torch.zeros_like(log_sigma).to(device)\n    kl_loss = torch.mean(kl_divergence(mu, log_sigma, mu2, log_sigma2))\n\n    return rec_loss + self.beta * kl_loss, \\\n           {'rec_loss': rec_loss, 'kl_loss': kl_loss}\n\n    \n  def reconstruct(self, x):\n    \"\"\"Use mean of posterior estimate for visualization reconstruction.\"\"\"\n    reconstruction = self.forward(x)['rec']\n    return reconstruction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ==== INIT MODEL\nnz = 128\nlearning_rate = 1e-3\n\nmodel = VAE(nz, cfg)\nmodel.to(device)\n# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.0005)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=7000,gamma=0.1)\ncriterion = nn.MSELoss(reduction=\"none\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pdb\n# ==== TRAIN LOOP\ntr_it = iter(train_dataloader)\nvl_it = iter(val_dataloader)\n\nprogress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\nlosses_train = []\nlosses_mean_train = []\nlosses_val = []\nlosses_mean_val = []\n\nfor itr in progress_bar:\n    try:\n        data = next(tr_it)\n    except StopIteration:\n        tr_it = iter(train_dataloader)\n        data = next(tr_it)\n    model.train()\n    torch.set_grad_enabled(True)\n\n    history_positions = data['history_positions'].to(device)\n    history_availabilities = data['history_availabilities'].to(device)\n    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n    targets_position = data[\"target_positions\"].to(device)\n\n    outputs = model(history_positions)\n\n    loss, losses = model.loss(targets_position, outputs, target_availabilities)\n    loss = loss.mean()\n\n    total_loss = losses['rec_loss'] + losses['kl_loss']\n    \n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    losses_train.append(total_loss.mean().item())\n    losses_mean_train.append(np.mean(losses_train))\n    \n    # Validation\n    if VALIDATION :\n        with torch.no_grad():\n            try:\n                val_data = next(vl_it)\n            except StopIteration:\n                vl_it = iter(val_dataloader)\n                val_data = next(vl_it)\n\n            model.eval()\n            # Forward pass\n            target_availabilities_val = val_data[\"target_availabilities\"].unsqueeze(-1).to(device)\n            targets_val = val_data[\"target_positions\"].to(device)\n            history_positions_val = val_data['history_positions'].to(device)\n            history_availabilities_val = data['history_availabilities'].to(device)\n\n            outputs_val = model(history_positions_val)\n                    \n            loss_v, losses_v = model.loss(targets_position, outputs, target_availabilities_val)\n\n            loss_v = loss_v.mean()\n\n            total_loss_val = losses_v['rec_loss'] + losses['kl_loss']\n            \n            losses_val.append(loss_v.item())\n\n            losses_mean_val.append(np.mean(losses_val))\n\n\n        desc = f\" TrainLoss: {round(loss.item(), 4)} ValLoss: {round(total_loss_val.mean().item(), 4)} TrainMeanLoss: {np.mean(losses_train)} ValMeanLoss: {np.mean(losses_val)}\" \n    else:\n        desc = f\" TrainLoss: {round(loss.item(), 4)}\"\n\n\n        #if len(losses_train)>0 and loss < min(losses_train):\n        #    print(f\"Loss improved from {min(losses_train)} to {loss}\")\n    lr_scheduler.step()\n\n    progress_bar.set_description(desc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_losses(train, val):\n    plt.subplot(2, 1, 1)\n    plt.plot(train, '-o')\n    plt.plot(val, '-o')\n    plt.xlabel('iterations')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of interations');\n    plt.gcf().set_size_inches(15, 12)\n\nplot_losses(losses_mean_train, losses_mean_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===== INIT DATASET\ntest_cfg = cfg[\"test_data_loader\"]\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\n# Test dataset/dataloader\ntest_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\ntest_mask = np.load(f\"../input/lyft-motion-prediction-autonomous-vehicles/scenes/mask.npz\")[\"arr_0\"]\ntest_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\n# test_dataset = AgentDataset(cfg, test_zarr, rasterizer)\ntest_dataloader = DataLoader(test_dataset,\n                             shuffle=test_cfg[\"shuffle\"],\n                             batch_size=test_cfg[\"batch_size\"],\n                             num_workers=test_cfg[\"num_workers\"])\n\n\nprint(test_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\nfuture_coords_offsets_pd = []\ntimestamps = []\nagent_ids = []\n\nwith torch.no_grad():\n    dataiter = tqdm(test_dataloader)\n    \n    for data in dataiter:\n\n        history_positions = data['history_positions'].to(device)\n\n        outputs = model(history_positions)\n                \n        future_coords_offsets_pd.append(outputs['rec'].cpu().numpy().copy())\n        timestamps.append(data[\"timestamp\"].numpy().copy())\n        agent_ids.append(data[\"track_id\"].numpy().copy())\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}